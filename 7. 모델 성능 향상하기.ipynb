{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 성능 향상하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.모델 하이퍼파라미터 튜닝\n",
    "- 그리드 서치\n",
    "- 랜덤 서치\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. 그리드 서치(Grid Search)\n",
    "- 가능한 모든 하이퍼파라미터 값의 조합에 대해 모델 성능을 측정하고 비교하면서 최적의 하이퍼파라미터를 찾는 방식\n",
    "- 모든 값을 탐색한다는 점에서 철저한 방식이지만 시간이 많이 소요되는 비효율적인 방법일 수 잇따."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Grid Search 실습(랜덤포레스트)\n",
    "- GridSearchCV 객체 생성 인자\n",
    "    - estimator \n",
    "    - param_grid : 튜닝하려는 하이퍼파라미터의 딕셔너리 지정\n",
    "    - scoring : 모델 성능 평가하는 전략, 기본값은 accuracy\n",
    "    - n_jobs : 병렬로 실행할 작업 수, -1: 모든 프로세서 사용, 기본값은 1\n",
    "    - refit : 가장 최적의 값을 찾은 후 입력된 estimator 객체를 해당 하이퍼파라미터로 재학습시키는 것으로, 기본값은 True\n",
    "    - cv : 교차 검증을 위한 fold 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "# 3차원의 이미지 행렬을 2차원으로 변경\n",
    "X_train = x_train.reshape(-1, 784)\n",
    "X_test = x_test.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eunjeong\\Documents\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "144 fits failed out of a total of 432.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "144 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Eunjeong\\Documents\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Eunjeong\\Documents\\Anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 340, in fit\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\Eunjeong\\Documents\\Anaconda\\Lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Eunjeong\\Documents\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of RandomForestClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Eunjeong\\Documents\\Anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.3941     0.52196667\n",
      " 0.57895    0.63236667 0.39723333 0.56116667 0.59498333 0.64475\n",
      "        nan        nan        nan        nan 0.42101667 0.5516\n",
      " 0.61201667 0.6407     0.42688333 0.54875    0.617      0.62823333\n",
      "        nan        nan        nan        nan 0.40245    0.51403333\n",
      " 0.60996667 0.62166667 0.39675    0.537      0.62886667 0.63593333\n",
      "        nan        nan        nan        nan 0.57243333 0.70416667\n",
      " 0.75528333 0.7797     0.55331667 0.69918333 0.75425    0.78535\n",
      "        nan        nan        nan        nan 0.55228333 0.70586667\n",
      " 0.75786667 0.78286667 0.57365    0.71148333 0.74991667 0.7802\n",
      "        nan        nan        nan        nan 0.61405    0.70325\n",
      " 0.74898333 0.78441667 0.55096667 0.70755    0.75251667 0.78308333\n",
      "        nan        nan        nan        nan 0.77181667 0.8665\n",
      " 0.89876667 0.91173333 0.7694     0.86531667 0.89576667 0.91148333\n",
      "        nan        nan        nan        nan 0.76813333 0.86653333\n",
      " 0.8974     0.91266667 0.75295    0.8675     0.89816667 0.9111\n",
      "        nan        nan        nan        nan 0.7579     0.86575\n",
      " 0.89841667 0.91268333 0.77001667 0.86488333 0.89893333 0.9119\n",
      "        nan        nan        nan        nan 0.79053333 0.86878333\n",
      " 0.91026667 0.9269     0.794      0.87028333 0.91113333 0.92973333\n",
      "        nan        nan        nan        nan 0.78738333 0.88325\n",
      " 0.91698333 0.93183333 0.78876667 0.87943333 0.91593333 0.93078333\n",
      "        nan        nan        nan        nan 0.79121667 0.88866667\n",
      " 0.91591667 0.93243333 0.7882     0.88468333 0.9172     0.93046667]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "estimator = RandomForestClassifier()\n",
    "\n",
    "param_grid = {'max_depth' : [3, 5, 10, None],\n",
    "              'n_estimators' : [1, 3, 5, 7],\n",
    "              'min_samples_leaf' : [1, 2, 3],\n",
    "              'min_samples_split' : [1, 2, 3]\n",
    "              }\n",
    "\n",
    "# 수행\n",
    "gs_cv = GridSearchCV( estimator=estimator,\n",
    "                     param_grid=param_grid,\n",
    "                     scoring='accuracy',\n",
    "                     cv=3\n",
    "                     )\n",
    "\n",
    "model = gs_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters : {'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 7}\n",
      "Best score : 0.9324333333333334\n"
     ]
    }
   ],
   "source": [
    "# 최적의 파라미터 및 성능 확인\n",
    "print(f'Best hyperparameters : {model.best_params_}')\n",
    "print(f'Best score : {model.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score : 0.9408\n"
     ]
    }
   ],
   "source": [
    "# 시험 테스트 및 성능 확인\n",
    "clf = model.best_estimator_\n",
    "clf.fit(X_train, y_train)\n",
    "print(f'test score : {clf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3.랜덤서치(Random Search)\n",
    "- 각 조합에 대해 훈련하고 점수를 매기는 대신 랜점 조합이 선택된다.\n",
    "- 시간 및 리소스 제약에 따라 검색 반복 횟수를 설정할 수 있으며, 파라미터 탐색 범위가 넓거나 연속적인 값을 탐색해야 하는 경우에 효율적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4.Random Search 실습(램덤 포레스트)\n",
    "- RandomSearchCV 객체 생성 인자\n",
    "    - estimator\n",
    "    - n_iter : 파라미터 검색 횟수, 지정한 횟수만큼만 조합을 반복하며 평가, 기본값은 10 \n",
    "    - param_distributions : 튜닝하려는 하이퍼파라미터의 딕셔너리 지정\n",
    "    - scoring : 모델 성능 평가하는 전략, 기본값은 accuracy\n",
    "    - n_jobs : 병렬로 실행할 작업 수, -1: 모든 프로세서 사용, 기본값은 1\n",
    "    - refit : 가장 최적의 값을 찾은 후 입력된 estimator 객체를 해당 하이퍼파라미터로 재학습시키는 것으로, 기본값은 True\n",
    "    - cv : 교차 검증을 위한 fold 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestClassifier()\n",
    "\n",
    "# 범위 지정\n",
    "param_distributions = {'max_depth' : list(np.arange(3, 13, step=3))+[None],\n",
    "                       'n_estimators' : np.arange(10, 320, step=100),\n",
    "                       'max_features' : randint(1,7),\n",
    "                       'criterion' : ['gini', 'entropy'],\n",
    "                       'min_samples_leaf' : randint(1,4),\n",
    "                       'min_samples_split' : np.arange(2, 8, step=2)\n",
    "                       }\n",
    "\n",
    "# 수행\n",
    "rs_cv = RandomizedSearchCV( estimator= estimator,\n",
    "                           param_distributions=param_distributions,\n",
    "                           n_iter=10,\n",
    "                           scoring='accuracy',\n",
    "                           n_jobs=1,\n",
    "                           refit=True,\n",
    "                           cv=3)\n",
    "\n",
    "model = rs_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters : {'criterion': 'gini', 'max_depth': 12, 'max_features': 5, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 110}\n",
      "Best score : 0.9398833333333334\n"
     ]
    }
   ],
   "source": [
    "# 최적의 파라미터 및 성능 확인\n",
    "print(f'Best hyperparameters : {model.best_params_}')\n",
    "print(f'Best score : {model.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score : 0.9454\n"
     ]
    }
   ],
   "source": [
    "# 시험 테스트 및 성능 확인\n",
    "clf = model.best_estimator_\n",
    "clf.fit(X_train, y_train)\n",
    "print(f'test score : {clf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.머신러닝 모델링 및 하이퍼파라미터 튜닝 실습\n",
    "- 항공권 가격 예측 데이터와 항공사 고객 만족 여부 데이터를 가지고 실습\n",
    "- 데이터 출처 : https://www.kaggle.com/datasets/shubhambathwal/flight-price-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.(회귀)항공권 가격 예측 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>airline</th>\n",
       "      <th>flight</th>\n",
       "      <th>source_city</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stops</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>destination_city</th>\n",
       "      <th>class</th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>SpiceJet</td>\n",
       "      <td>SG-8709</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Evening</td>\n",
       "      <td>zero</td>\n",
       "      <td>Night</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SpiceJet</td>\n",
       "      <td>SG-8157</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AirAsia</td>\n",
       "      <td>I5-764</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>5956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-995</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1</td>\n",
       "      <td>5955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   airline   flight source_city departure_time stops  \\\n",
       "0           0  SpiceJet  SG-8709       Delhi        Evening  zero   \n",
       "1           1  SpiceJet  SG-8157       Delhi  Early_Morning  zero   \n",
       "2           2   AirAsia   I5-764       Delhi  Early_Morning  zero   \n",
       "3           3   Vistara   UK-995       Delhi        Morning  zero   \n",
       "\n",
       "    arrival_time destination_city    class  duration  days_left  price  \n",
       "0          Night           Mumbai  Economy      2.17          1   5953  \n",
       "1        Morning           Mumbai  Economy      2.33          1   5953  \n",
       "2  Early_Morning           Mumbai  Economy      2.17          1   5956  \n",
       "3      Afternoon           Mumbai  Economy      2.25          1   5955  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cdf = pd.read_csv('./Flight_Price_Prediction/Clean_Dataset.csv', encoding='cp949')\n",
    "\n",
    "# 5000건만 사용\n",
    "cdf = cdf[:5000]\n",
    "\n",
    "cdf.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>flight</th>\n",
       "      <th>source_city</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stops</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>destination_city</th>\n",
       "      <th>class</th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SpiceJet</td>\n",
       "      <td>SG-8709</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Evening</td>\n",
       "      <td>zero</td>\n",
       "      <td>Night</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SpiceJet</td>\n",
       "      <td>SG-8157</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AirAsia</td>\n",
       "      <td>I5-764</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>5956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-995</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1</td>\n",
       "      <td>5955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-963</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>5955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    airline   flight source_city departure_time stops   arrival_time  \\\n",
       "0  SpiceJet  SG-8709       Delhi        Evening  zero          Night   \n",
       "1  SpiceJet  SG-8157       Delhi  Early_Morning  zero        Morning   \n",
       "2   AirAsia   I5-764       Delhi  Early_Morning  zero  Early_Morning   \n",
       "3   Vistara   UK-995       Delhi        Morning  zero      Afternoon   \n",
       "4   Vistara   UK-963       Delhi        Morning  zero        Morning   \n",
       "\n",
       "  destination_city    class  duration  days_left  price  \n",
       "0           Mumbai  Economy      2.17          1   5953  \n",
       "1           Mumbai  Economy      2.33          1   5953  \n",
       "2           Mumbai  Economy      2.17          1   5956  \n",
       "3           Mumbai  Economy      2.25          1   5955  \n",
       "4           Mumbai  Economy      2.33          1   5955  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unnamed 컬럼 확인\n",
    "cdf['Unnamed: 0'].value_counts().min()\n",
    "cdf['Unnamed: 0'].value_counts().max()\n",
    "\n",
    "# 인덱스이므로 삭제\n",
    "cdf.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# 결과 확인\n",
    "cdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>flight</th>\n",
       "      <th>source_city</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stops</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>destination_city</th>\n",
       "      <th>class</th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6</td>\n",
       "      <td>222</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-819</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Evening</td>\n",
       "      <td>one</td>\n",
       "      <td>Night</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1496</td>\n",
       "      <td>90</td>\n",
       "      <td>5000</td>\n",
       "      <td>1391</td>\n",
       "      <td>3619</td>\n",
       "      <td>1702</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.665682</td>\n",
       "      <td>14.216800</td>\n",
       "      <td>7589.786600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.247512</td>\n",
       "      <td>7.109536</td>\n",
       "      <td>4476.362204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2409.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.330000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4678.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.670000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>5955.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.080000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10549.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.080000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>31260.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        airline  flight source_city departure_time stops arrival_time  \\\n",
       "count      5000    5000        5000           5000  5000         5000   \n",
       "unique        6     222           1              6     3            6   \n",
       "top     Vistara  UK-819       Delhi        Evening   one        Night   \n",
       "freq       1496      90        5000           1391  3619         1702   \n",
       "mean        NaN     NaN         NaN            NaN   NaN          NaN   \n",
       "std         NaN     NaN         NaN            NaN   NaN          NaN   \n",
       "min         NaN     NaN         NaN            NaN   NaN          NaN   \n",
       "25%         NaN     NaN         NaN            NaN   NaN          NaN   \n",
       "50%         NaN     NaN         NaN            NaN   NaN          NaN   \n",
       "75%         NaN     NaN         NaN            NaN   NaN          NaN   \n",
       "max         NaN     NaN         NaN            NaN   NaN          NaN   \n",
       "\n",
       "       destination_city    class     duration    days_left         price  \n",
       "count              5000     5000  5000.000000  5000.000000   5000.000000  \n",
       "unique                1        1          NaN          NaN           NaN  \n",
       "top              Mumbai  Economy          NaN          NaN           NaN  \n",
       "freq               5000     5000          NaN          NaN           NaN  \n",
       "mean                NaN      NaN     9.665682    14.216800   7589.786600  \n",
       "std                 NaN      NaN     7.247512     7.109536   4476.362204  \n",
       "min                 NaN      NaN     2.000000     1.000000   2409.000000  \n",
       "25%                 NaN      NaN     2.330000     8.000000   4678.000000  \n",
       "50%                 NaN      NaN     7.670000    14.000000   5955.000000  \n",
       "75%                 NaN      NaN    14.080000    20.000000  10549.000000  \n",
       "max                 NaN      NaN    30.080000    26.000000  31260.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf.describe()\n",
    "cdf.describe(include='all')\n",
    "\n",
    "# 수치 데이터 : duration, day_left, price\n",
    "# 문자형에 대한 정보 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   airline           5000 non-null   object \n",
      " 1   flight            5000 non-null   object \n",
      " 2   source_city       5000 non-null   object \n",
      " 3   departure_time    5000 non-null   object \n",
      " 4   stops             5000 non-null   object \n",
      " 5   arrival_time      5000 non-null   object \n",
      " 6   destination_city  5000 non-null   object \n",
      " 7   class             5000 non-null   object \n",
      " 8   duration          5000 non-null   float64\n",
      " 9   days_left         5000 non-null   int64  \n",
      " 10  price             5000 non-null   int64  \n",
      "dtypes: float64(1), int64(2), object(8)\n",
      "memory usage: 429.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# null확인\n",
    "cdf.info()\n",
    "\n",
    "# null값 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vistara      1496\n",
       "Air_India    1311\n",
       "Indigo        813\n",
       "GO_FIRST      801\n",
       "SpiceJet      296\n",
       "AirAsia       283\n",
       "Name: airline, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf.airline.value_counts()\n",
    "\n",
    "# 6개의 항공사로 가격과 관련 있는지 바로 그래프를 통해 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "ax = sns.barplot(x='airline', y='price', data=cdf)\n",
    "\n",
    "ax.set(title='airline & price')\n",
    "plt.show()\n",
    "\n",
    "# 항공사별로 가격이 다름(Air_india > Vistara > GO_FIRST > Indigo > Spicejet > AirAsia)\n",
    "# 항공사가 가격에 영향을 미치므로 해당 컬럼 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UK-819     90\n",
       "UK-879     62\n",
       "UK-899     61\n",
       "UK-705     61\n",
       "UK-835     60\n",
       "           ..\n",
       "AI-9939     2\n",
       "I5-881      2\n",
       "I5-744      1\n",
       "SG-9974     1\n",
       "SG-8339     1\n",
       "Name: flight, Length: 222, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf.head(1)\n",
    "\n",
    "cdf.flight.value_counts()\n",
    "# 1. 3자리 코드와 4자리 코드는 유의미한 데이터인가?\n",
    "    # 국내선과 국제선의 차이. \n",
    "# 2. 코드 패턴은 유의미한가?\n",
    "    # 항공편명에 따른 코드 부여. \n",
    "# --> 항공평명(UK, AI ...)은 유의미하나 출발지, 도착지, 거리 등의 데이터가 있으므로 flight를 대체할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>source_city</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stops</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>destination_city</th>\n",
       "      <th>class</th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SpiceJet</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Evening</td>\n",
       "      <td>zero</td>\n",
       "      <td>Night</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SpiceJet</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AirAsia</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>5956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1</td>\n",
       "      <td>5955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>5955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    airline source_city departure_time stops   arrival_time destination_city  \\\n",
       "0  SpiceJet       Delhi        Evening  zero          Night           Mumbai   \n",
       "1  SpiceJet       Delhi  Early_Morning  zero        Morning           Mumbai   \n",
       "2   AirAsia       Delhi  Early_Morning  zero  Early_Morning           Mumbai   \n",
       "3   Vistara       Delhi        Morning  zero      Afternoon           Mumbai   \n",
       "4   Vistara       Delhi        Morning  zero        Morning           Mumbai   \n",
       "\n",
       "     class  duration  days_left  price  \n",
       "0  Economy      2.17          1   5953  \n",
       "1  Economy      2.33          1   5953  \n",
       "2  Economy      2.17          1   5956  \n",
       "3  Economy      2.25          1   5955  \n",
       "4  Economy      2.33          1   5955  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf.drop('flight', axis=1, inplace=True)\n",
    "cdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   airline           5000 non-null   object \n",
      " 1   source_city       5000 non-null   object \n",
      " 2   departure_time    5000 non-null   object \n",
      " 3   stops             5000 non-null   object \n",
      " 4   arrival_time      5000 non-null   object \n",
      " 5   destination_city  5000 non-null   object \n",
      " 6   class             5000 non-null   object \n",
      " 7   duration          5000 non-null   float64\n",
      " 8   days_left         5000 non-null   int64  \n",
      " 9   price             5000 non-null   int64  \n",
      "dtypes: float64(1), int64(2), object(7)\n",
      "memory usage: 390.8+ KB\n",
      "(5000, 20)\n"
     ]
    }
   ],
   "source": [
    "# 웟핫 인코딩\n",
    "    # 범주형 데이터의 수치화, 수치값인데 문자형으로 저장된 것은 없는지, 수치형 변수 중 실제로 범주형이지만 수치형인 것은 없는지\n",
    "cdf.info()\n",
    "\n",
    "# --> 없음\n",
    "\n",
    "# 실행하기\n",
    "dummies_cdf = pd.get_dummies(cdf,\n",
    "                             # 범주형 데이터만\n",
    "                             columns=['airline', 'source_city', 'departure_time', 'stops', 'arrival_time',\n",
    "                                      'destination_city', 'class'],\n",
    "                             drop_first=True #맨 첫번째 범주에 1000으로 하는 것이 아니라 0000을 부여해서 별도의 dummy variable을 부여하지 않고 다른 것들의 여집합으로 표시함\n",
    "                             )\n",
    "\n",
    "dummies_cdf.head()\n",
    "\n",
    "print(dummies_cdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. 학습 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 19) (5000,)\n"
     ]
    }
   ],
   "source": [
    "y = dummies_cdf.price\n",
    "x = dummies_cdf.drop('price', axis=1)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. 모델 생성 및 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost\n",
    "# !pip install lightbgm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "from xgboost import XGBRFRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7가지 모델로 학습\n",
    "\n",
    "# 모델 생성\n",
    "# n_jobs : 학습할 때 사용할 cpu개수(모든 cpu : -1)\n",
    "# random_state : 동일한 학습을 위한 고정값\n",
    "lr = LinearRegression(n_jobs=-1)\n",
    "dtr = DecisionTreeRegressor( random_state=1)\n",
    "rfr = RandomForestRegressor( random_state=1)\n",
    "gbr = GradientBoostingRegressor( random_state=1)\n",
    "xgbr = XGBRFRegressor(n_jobs=-1, random_state=1)\n",
    "etr = ExtraTreesRegressor(n_jobs=-1, random_state=1)\n",
    "lgbmr = LGBMRegressor(n_jobs=-1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 19) (3500,)\n",
      "(1500, 19) (1500,)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=2023)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-5. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 3500, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7548.349714\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(n_jobs=-1, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(n_jobs=-1, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(n_jobs=-1, random_state=1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(x_train, y_train)\n",
    "dtr.fit(x_train, y_train)\n",
    "rfr.fit(x_train, y_train)\n",
    "gbr.fit(x_train, y_train)\n",
    "xgbr.fit(x_train, y_train)\n",
    "etr.fit(x_train, y_train)\n",
    "lgbmr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-6. 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>dtr</th>\n",
       "      <th>rfr</th>\n",
       "      <th>gbr</th>\n",
       "      <th>xgbr</th>\n",
       "      <th>etr</th>\n",
       "      <th>lgbmr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.61523</td>\n",
       "      <td>0.70927</td>\n",
       "      <td>0.79828</td>\n",
       "      <td>0.76376</td>\n",
       "      <td>0.73031</td>\n",
       "      <td>0.74807</td>\n",
       "      <td>0.79957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>2818.92861</td>\n",
       "      <td>2450.35112</td>\n",
       "      <td>2041.08811</td>\n",
       "      <td>2208.84675</td>\n",
       "      <td>2360.00532</td>\n",
       "      <td>2280.97506</td>\n",
       "      <td>2034.53417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              lr         dtr         rfr         gbr        xgbr         etr  \\\n",
       "r2       0.61523     0.70927     0.79828     0.76376     0.73031     0.74807   \n",
       "rmse  2818.92861  2450.35112  2041.08811  2208.84675  2360.00532  2280.97506   \n",
       "\n",
       "           lgbmr  \n",
       "r2       0.79957  \n",
       "rmse  2034.53417  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "models = [lr, dtr, rfr, gbr, xgbr, etr, lgbmr]\n",
    "r2_list = []\n",
    "rmse_list = []\n",
    "\n",
    "for model in models:\n",
    "    pred = model.predict(x_test)\n",
    "    r2_list.append( round(r2_score(y_test, pred), 5) )\n",
    "    rmse_list.append( round(mean_squared_error(y_test, pred, squared=False) , 5) )\n",
    "    \n",
    "r2_score_df = pd.DataFrame( [r2_list, rmse_list],\n",
    "                            columns=['lr', 'dtr', 'rfr', 'gbr', 'xgbr', 'etr', 'lgbmr'],\n",
    "                            index = ['r2', 'rmse'])\n",
    "\n",
    "r2_score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> lgbmr이 가장 성능이 높으므로 이 모델을 기반으로 성능 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-7. 최적을 하이퍼파라미터 찾기(GridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000092 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000113 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000181 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000171 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000163 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000064 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000161 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000142 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000171 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 3500, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7548.349714\n",
      "CPU times: total: 1min 20s\n",
      "Wall time: 6.47 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=LGBMRegressor(n_jobs=-1, random_state=1),\n",
       "             param_grid={&#x27;colsample_bytree&#x27;: [0.5, 0.7],\n",
       "                         &#x27;learning_rate&#x27;: [0.1, 0.01, 0.003],\n",
       "                         &#x27;max_depth&#x27;: [20, 30, 40]},\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=LGBMRegressor(n_jobs=-1, random_state=1),\n",
       "             param_grid={&#x27;colsample_bytree&#x27;: [0.5, 0.7],\n",
       "                         &#x27;learning_rate&#x27;: [0.1, 0.01, 0.003],\n",
       "                         &#x27;max_depth&#x27;: [20, 30, 40]},\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(n_jobs=-1, random_state=1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(n_jobs=-1, random_state=1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LGBMRegressor(n_jobs=-1, random_state=1),\n",
       "             param_grid={'colsample_bytree': [0.5, 0.7],\n",
       "                         'learning_rate': [0.1, 0.01, 0.003],\n",
       "                         'max_depth': [20, 30, 40]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.003],\n",
    "    'colsample_bytree' : [0.5, 0.7],\n",
    "    'max_depth' : [20, 30, 40]    \n",
    "}\n",
    "\n",
    "# 최적의 값 검색\n",
    "cv_lgbmr = GridSearchCV(estimator=lgbmr,\n",
    "                        param_grid=param_grid,\n",
    "                        cv=5, \n",
    "                        verbose=1)\n",
    "\n",
    "cv_lgbmr.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 30}\n",
      "0.803203388259023\n"
     ]
    }
   ],
   "source": [
    "#최적의 조합\n",
    "print(cv_lgbmr.best_params_)\n",
    "print(cv_lgbmr.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 3500, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7548.349714\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(colsample_bytree=0.7, max_depth=30, n_jobs=-1, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(colsample_bytree=0.7, max_depth=30, n_jobs=-1, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(colsample_bytree=0.7, max_depth=30, n_jobs=-1, random_state=1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 머신러닝 모델 검증\n",
    "# 최적의 하이퍼파라미터로 재학습\n",
    "best_lgbmr = LGBMRegressor(max_depth=30,\n",
    "                           colsample_bytree=0.7,\n",
    "                           learning_rate=0.1,\n",
    "                           n_jobs=-1,\n",
    "                           random_state=1)\n",
    "\n",
    "best_lgbmr.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-8. 성능 확인 및 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "r2 :  0.80154\n",
      "rmse :  2024.49371\n"
     ]
    }
   ],
   "source": [
    "# 모델 성능\n",
    "best_pred = best_lgbmr.predict(x_test)\n",
    "print('r2 : ', round(r2_score(y_test, best_pred), 5) )\n",
    "print('rmse : ', round(mean_squared_error(y_test, best_pred, squared=False), 5) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> R2 : 0.79957 / RMSE : 2034.53417 에서 R2 : 0.80154 / RMSE : 2024.49371 으로 성능이 향상됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-9. Random Search 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000125 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000134 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000105 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000133 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000170 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000108 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7601.929643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7576.702500\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 221\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7485.184643\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000128 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7503.671429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7574.260357\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 3500, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7548.349714\n",
      "CPU times: total: 1min 18s\n",
      "Wall time: 6.51 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=LGBMRegressor(n_jobs=-1, random_state=1),\n",
       "                   n_iter=500,\n",
       "                   param_distributions={&#x27;colsample_bytree&#x27;: [0.5, 0.7],\n",
       "                                        &#x27;learning_rate&#x27;: [0.1, 0.01, 0.003],\n",
       "                                        &#x27;max_depth&#x27;: [20, 30, 40]},\n",
       "                   verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=LGBMRegressor(n_jobs=-1, random_state=1),\n",
       "                   n_iter=500,\n",
       "                   param_distributions={&#x27;colsample_bytree&#x27;: [0.5, 0.7],\n",
       "                                        &#x27;learning_rate&#x27;: [0.1, 0.01, 0.003],\n",
       "                                        &#x27;max_depth&#x27;: [20, 30, 40]},\n",
       "                   verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(n_jobs=-1, random_state=1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(n_jobs=-1, random_state=1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=LGBMRegressor(n_jobs=-1, random_state=1),\n",
       "                   n_iter=500,\n",
       "                   param_distributions={'colsample_bytree': [0.5, 0.7],\n",
       "                                        'learning_rate': [0.1, 0.01, 0.003],\n",
       "                                        'max_depth': [20, 30, 40]},\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_dists = {\n",
    "    'learning_rate': [0.1, 0.01, 0.003],\n",
    "    'colsample_bytree' : [0.5, 0.7],\n",
    "    'max_depth' : [20, 30, 40]    \n",
    "}\n",
    "\n",
    "# 최적의 값 검색\n",
    "cv_lgbmr = RandomizedSearchCV(estimator=lgbmr,\n",
    "                        param_distributions=param_dists,\n",
    "                        n_iter = 500,\n",
    "                        cv=5, \n",
    "                        verbose=1)\n",
    "\n",
    "cv_lgbmr.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 30, 'learning_rate': 0.1, 'colsample_bytree': 0.7}\n",
      "0.803203388259023\n"
     ]
    }
   ],
   "source": [
    "# 성능 확인\n",
    "print(cv_lgbmr.best_params_)\n",
    "print(cv_lgbmr.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.(분류)항공사 고객만족 여부 예측 모델링\n",
    "- 데이터 출처 : https://www.kaggle.com/datasets/sjleshrac/airlines-customer-satisfaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1.데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Customer Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Type of Travel</th>\n",
       "      <th>Class</th>\n",
       "      <th>Flight Distance</th>\n",
       "      <th>Seat comfort</th>\n",
       "      <th>Departure/Arrival time convenient</th>\n",
       "      <th>Food and drink</th>\n",
       "      <th>Gate location</th>\n",
       "      <th>Inflight wifi service</th>\n",
       "      <th>Inflight entertainment</th>\n",
       "      <th>Online support</th>\n",
       "      <th>Ease of Online booking</th>\n",
       "      <th>On-board service</th>\n",
       "      <th>Leg room service</th>\n",
       "      <th>Baggage handling</th>\n",
       "      <th>Checkin service</th>\n",
       "      <th>Cleanliness</th>\n",
       "      <th>Online boarding</th>\n",
       "      <th>Departure Delay in Minutes</th>\n",
       "      <th>Arrival Delay in Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>dissatisfied</td>\n",
       "      <td>Male</td>\n",
       "      <td>disloyal Customer</td>\n",
       "      <td>25</td>\n",
       "      <td>Business travel</td>\n",
       "      <td>Business</td>\n",
       "      <td>1880</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50001</th>\n",
       "      <td>dissatisfied</td>\n",
       "      <td>Female</td>\n",
       "      <td>disloyal Customer</td>\n",
       "      <td>25</td>\n",
       "      <td>Business travel</td>\n",
       "      <td>Business</td>\n",
       "      <td>2144</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50002</th>\n",
       "      <td>dissatisfied</td>\n",
       "      <td>Female</td>\n",
       "      <td>disloyal Customer</td>\n",
       "      <td>25</td>\n",
       "      <td>Business travel</td>\n",
       "      <td>Business</td>\n",
       "      <td>2068</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50003</th>\n",
       "      <td>dissatisfied</td>\n",
       "      <td>Female</td>\n",
       "      <td>disloyal Customer</td>\n",
       "      <td>25</td>\n",
       "      <td>Business travel</td>\n",
       "      <td>Business</td>\n",
       "      <td>1630</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50004</th>\n",
       "      <td>dissatisfied</td>\n",
       "      <td>Female</td>\n",
       "      <td>disloyal Customer</td>\n",
       "      <td>25</td>\n",
       "      <td>Business travel</td>\n",
       "      <td>Business</td>\n",
       "      <td>1358</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       satisfaction  Gender      Customer Type  Age   Type of Travel  \\\n",
       "50000  dissatisfied    Male  disloyal Customer   25  Business travel   \n",
       "50001  dissatisfied  Female  disloyal Customer   25  Business travel   \n",
       "50002  dissatisfied  Female  disloyal Customer   25  Business travel   \n",
       "50003  dissatisfied  Female  disloyal Customer   25  Business travel   \n",
       "50004  dissatisfied  Female  disloyal Customer   25  Business travel   \n",
       "\n",
       "          Class  Flight Distance  Seat comfort  \\\n",
       "50000  Business             1880             2   \n",
       "50001  Business             2144             2   \n",
       "50002  Business             2068             2   \n",
       "50003  Business             1630             2   \n",
       "50004  Business             1358             2   \n",
       "\n",
       "       Departure/Arrival time convenient  Food and drink  Gate location  \\\n",
       "50000                                  4               2              3   \n",
       "50001                                  4               2              3   \n",
       "50002                                  4               2              3   \n",
       "50003                                  4               2              3   \n",
       "50004                                  4               2              3   \n",
       "\n",
       "       Inflight wifi service  Inflight entertainment  Online support  \\\n",
       "50000                      4                       2               4   \n",
       "50001                      3                       2               1   \n",
       "50002                      3                       2               3   \n",
       "50003                      1                       2               1   \n",
       "50004                      2                       2               2   \n",
       "\n",
       "       Ease of Online booking  On-board service  Leg room service  \\\n",
       "50000                       4                 4                 2   \n",
       "50001                       3                 4                 5   \n",
       "50002                       3                 3                 4   \n",
       "50003                       1                 4                 5   \n",
       "50004                       2                 3                 4   \n",
       "\n",
       "       Baggage handling  Checkin service  Cleanliness  Online boarding  \\\n",
       "50000                 4                4            5                4   \n",
       "50001                 4                3            5                3   \n",
       "50002                 4                5            5                3   \n",
       "50003                 5                4            4                1   \n",
       "50004                 4                5            4                2   \n",
       "\n",
       "       Departure Delay in Minutes  Arrival Delay in Minutes  \n",
       "50000                           0                       1.0  \n",
       "50001                           0                       0.0  \n",
       "50002                           0                       0.0  \n",
       "50003                           0                       2.0  \n",
       "50004                          38                      36.0  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf = pd.read_csv('./지도학습_데이터/Invistico_Airline.csv')\n",
    "\n",
    "# 5000건만 사용\n",
    "cdf = cdf[50000:100000]\n",
    "\n",
    "cdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Customer Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Type of Travel</th>\n",
       "      <th>Class</th>\n",
       "      <th>Flight Distance</th>\n",
       "      <th>Seat comfort</th>\n",
       "      <th>Departure/Arrival time convenient</th>\n",
       "      <th>Food and drink</th>\n",
       "      <th>Gate location</th>\n",
       "      <th>Inflight wifi service</th>\n",
       "      <th>Inflight entertainment</th>\n",
       "      <th>Online support</th>\n",
       "      <th>Ease of Online booking</th>\n",
       "      <th>On-board service</th>\n",
       "      <th>Leg room service</th>\n",
       "      <th>Baggage handling</th>\n",
       "      <th>Checkin service</th>\n",
       "      <th>Cleanliness</th>\n",
       "      <th>Online boarding</th>\n",
       "      <th>Departure Delay in Minutes</th>\n",
       "      <th>Arrival Delay in Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>dissatisfied</td>\n",
       "      <td>Male</td>\n",
       "      <td>disloyal Customer</td>\n",
       "      <td>25</td>\n",
       "      <td>Business travel</td>\n",
       "      <td>Business</td>\n",
       "      <td>1880</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50001</th>\n",
       "      <td>dissatisfied</td>\n",
       "      <td>Female</td>\n",
       "      <td>disloyal Customer</td>\n",
       "      <td>25</td>\n",
       "      <td>Business travel</td>\n",
       "      <td>Business</td>\n",
       "      <td>2144</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50002</th>\n",
       "      <td>dissatisfied</td>\n",
       "      <td>Female</td>\n",
       "      <td>disloyal Customer</td>\n",
       "      <td>25</td>\n",
       "      <td>Business travel</td>\n",
       "      <td>Business</td>\n",
       "      <td>2068</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50003</th>\n",
       "      <td>dissatisfied</td>\n",
       "      <td>Female</td>\n",
       "      <td>disloyal Customer</td>\n",
       "      <td>25</td>\n",
       "      <td>Business travel</td>\n",
       "      <td>Business</td>\n",
       "      <td>1630</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50004</th>\n",
       "      <td>dissatisfied</td>\n",
       "      <td>Female</td>\n",
       "      <td>disloyal Customer</td>\n",
       "      <td>25</td>\n",
       "      <td>Business travel</td>\n",
       "      <td>Business</td>\n",
       "      <td>1358</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       satisfaction  Gender      Customer Type  Age   Type of Travel  \\\n",
       "50000  dissatisfied    Male  disloyal Customer   25  Business travel   \n",
       "50001  dissatisfied  Female  disloyal Customer   25  Business travel   \n",
       "50002  dissatisfied  Female  disloyal Customer   25  Business travel   \n",
       "50003  dissatisfied  Female  disloyal Customer   25  Business travel   \n",
       "50004  dissatisfied  Female  disloyal Customer   25  Business travel   \n",
       "\n",
       "          Class  Flight Distance  Seat comfort  \\\n",
       "50000  Business             1880             2   \n",
       "50001  Business             2144             2   \n",
       "50002  Business             2068             2   \n",
       "50003  Business             1630             2   \n",
       "50004  Business             1358             2   \n",
       "\n",
       "       Departure/Arrival time convenient  Food and drink  Gate location  \\\n",
       "50000                                  4               2              3   \n",
       "50001                                  4               2              3   \n",
       "50002                                  4               2              3   \n",
       "50003                                  4               2              3   \n",
       "50004                                  4               2              3   \n",
       "\n",
       "       Inflight wifi service  Inflight entertainment  Online support  \\\n",
       "50000                      4                       2               4   \n",
       "50001                      3                       2               1   \n",
       "50002                      3                       2               3   \n",
       "50003                      1                       2               1   \n",
       "50004                      2                       2               2   \n",
       "\n",
       "       Ease of Online booking  On-board service  Leg room service  \\\n",
       "50000                       4                 4                 2   \n",
       "50001                       3                 4                 5   \n",
       "50002                       3                 3                 4   \n",
       "50003                       1                 4                 5   \n",
       "50004                       2                 3                 4   \n",
       "\n",
       "       Baggage handling  Checkin service  Cleanliness  Online boarding  \\\n",
       "50000                 4                4            5                4   \n",
       "50001                 4                3            5                3   \n",
       "50002                 4                5            5                3   \n",
       "50003                 5                4            4                1   \n",
       "50004                 4                5            4                2   \n",
       "\n",
       "       Departure Delay in Minutes  Arrival Delay in Minutes  \n",
       "50000                           0                       1.0  \n",
       "50001                           0                       0.0  \n",
       "50002                           0                       0.0  \n",
       "50003                           0                       2.0  \n",
       "50004                          38                      36.0  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 칼럼 보기\n",
    "pd.set_option('display.max_columns', None)\n",
    "cdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dissatisfied    26903\n",
       "satisfied       23097\n",
       "Name: satisfaction, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 타깃 변수 satisfation. 칼럼의 레이블별 개수 확인\n",
    "cdf.satisfaction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "satisfaction                           0\n",
       "Gender                                 0\n",
       "Customer Type                          0\n",
       "Age                                    0\n",
       "Type of Travel                         0\n",
       "Class                                  0\n",
       "Flight Distance                        0\n",
       "Seat comfort                           0\n",
       "Departure/Arrival time convenient      0\n",
       "Food and drink                         0\n",
       "Gate location                          0\n",
       "Inflight wifi service                  0\n",
       "Inflight entertainment                 0\n",
       "Online support                         0\n",
       "Ease of Online booking                 0\n",
       "On-board service                       0\n",
       "Leg room service                       0\n",
       "Baggage handling                       0\n",
       "Checkin service                        0\n",
       "Cleanliness                            0\n",
       "Online boarding                        0\n",
       "Departure Delay in Minutes             0\n",
       "Arrival Delay in Minutes             141\n",
       "dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null확인\n",
    "cdf.isnull().sum()\n",
    "\n",
    "# print(141*100/50000) # 141개의 행은 전체 데이터의 0.28%.로 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "satisfaction                         0\n",
       "Gender                               0\n",
       "Customer Type                        0\n",
       "Age                                  0\n",
       "Type of Travel                       0\n",
       "Class                                0\n",
       "Flight Distance                      0\n",
       "Seat comfort                         0\n",
       "Departure/Arrival time convenient    0\n",
       "Food and drink                       0\n",
       "Gate location                        0\n",
       "Inflight wifi service                0\n",
       "Inflight entertainment               0\n",
       "Online support                       0\n",
       "Ease of Online booking               0\n",
       "On-board service                     0\n",
       "Leg room service                     0\n",
       "Baggage handling                     0\n",
       "Checkin service                      0\n",
       "Cleanliness                          0\n",
       "Online boarding                      0\n",
       "Departure Delay in Minutes           0\n",
       "Arrival Delay in Minutes             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf.dropna(axis=0, inplace=True)\n",
    "cdf.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3.학습 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49859, 22) (49859,)\n"
     ]
    }
   ],
   "source": [
    "y = cdf['satisfaction']\n",
    "x = cdf.drop('satisfaction', axis=1)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4.원핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 49859 entries, 50000 to 99999\n",
      "Data columns (total 22 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   Gender                             49859 non-null  object \n",
      " 1   Customer Type                      49859 non-null  object \n",
      " 2   Age                                49859 non-null  int64  \n",
      " 3   Type of Travel                     49859 non-null  object \n",
      " 4   Class                              49859 non-null  object \n",
      " 5   Flight Distance                    49859 non-null  int64  \n",
      " 6   Seat comfort                       49859 non-null  int64  \n",
      " 7   Departure/Arrival time convenient  49859 non-null  int64  \n",
      " 8   Food and drink                     49859 non-null  int64  \n",
      " 9   Gate location                      49859 non-null  int64  \n",
      " 10  Inflight wifi service              49859 non-null  int64  \n",
      " 11  Inflight entertainment             49859 non-null  int64  \n",
      " 12  Online support                     49859 non-null  int64  \n",
      " 13  Ease of Online booking             49859 non-null  int64  \n",
      " 14  On-board service                   49859 non-null  int64  \n",
      " 15  Leg room service                   49859 non-null  int64  \n",
      " 16  Baggage handling                   49859 non-null  int64  \n",
      " 17  Checkin service                    49859 non-null  int64  \n",
      " 18  Cleanliness                        49859 non-null  int64  \n",
      " 19  Online boarding                    49859 non-null  int64  \n",
      " 20  Departure Delay in Minutes         49859 non-null  int64  \n",
      " 21  Arrival Delay in Minutes           49859 non-null  float64\n",
      "dtypes: float64(1), int64(17), object(4)\n",
      "memory usage: 8.7+ MB\n"
     ]
    }
   ],
   "source": [
    "x.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 49859 entries, 50000 to 99999\n",
      "Data columns (total 26 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   Age                                49859 non-null  int64  \n",
      " 1   Flight Distance                    49859 non-null  int64  \n",
      " 2   Seat comfort                       49859 non-null  int64  \n",
      " 3   Departure/Arrival time convenient  49859 non-null  int64  \n",
      " 4   Food and drink                     49859 non-null  int64  \n",
      " 5   Gate location                      49859 non-null  int64  \n",
      " 6   Inflight wifi service              49859 non-null  int64  \n",
      " 7   Inflight entertainment             49859 non-null  int64  \n",
      " 8   Online support                     49859 non-null  int64  \n",
      " 9   Ease of Online booking             49859 non-null  int64  \n",
      " 10  On-board service                   49859 non-null  int64  \n",
      " 11  Leg room service                   49859 non-null  int64  \n",
      " 12  Baggage handling                   49859 non-null  int64  \n",
      " 13  Checkin service                    49859 non-null  int64  \n",
      " 14  Cleanliness                        49859 non-null  int64  \n",
      " 15  Online boarding                    49859 non-null  int64  \n",
      " 16  Departure Delay in Minutes         49859 non-null  int64  \n",
      " 17  Arrival Delay in Minutes           49859 non-null  float64\n",
      " 18  Gender_Female                      49859 non-null  uint8  \n",
      " 19  Gender_Male                        49859 non-null  uint8  \n",
      " 20  Customer Type_Loyal Customer       49859 non-null  uint8  \n",
      " 21  Customer Type_disloyal Customer    49859 non-null  uint8  \n",
      " 22  Type of Travel_Business travel     49859 non-null  uint8  \n",
      " 23  Class_Business                     49859 non-null  uint8  \n",
      " 24  Class_Eco                          49859 non-null  uint8  \n",
      " 25  Class_Eco Plus                     49859 non-null  uint8  \n",
      "dtypes: float64(1), int64(17), uint8(8)\n",
      "memory usage: 7.6 MB\n"
     ]
    }
   ],
   "source": [
    "# object -> 원핫인코딩\n",
    "x_gd = pd.get_dummies(x,\n",
    "                      columns=['Gender', 'Customer Type', 'Type of Travel', 'Class'],\n",
    "                      drop_first=False)\n",
    "\n",
    "x_gd.info() # -> object없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Flight Distance</th>\n",
       "      <th>Seat comfort</th>\n",
       "      <th>Departure/Arrival time convenient</th>\n",
       "      <th>Food and drink</th>\n",
       "      <th>Gate location</th>\n",
       "      <th>Inflight wifi service</th>\n",
       "      <th>Inflight entertainment</th>\n",
       "      <th>Online support</th>\n",
       "      <th>Ease of Online booking</th>\n",
       "      <th>On-board service</th>\n",
       "      <th>Leg room service</th>\n",
       "      <th>Baggage handling</th>\n",
       "      <th>Checkin service</th>\n",
       "      <th>Cleanliness</th>\n",
       "      <th>Online boarding</th>\n",
       "      <th>Departure Delay in Minutes</th>\n",
       "      <th>Arrival Delay in Minutes</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Customer Type_Loyal Customer</th>\n",
       "      <th>Customer Type_disloyal Customer</th>\n",
       "      <th>Type of Travel_Business travel</th>\n",
       "      <th>Class_Business</th>\n",
       "      <th>Class_Eco</th>\n",
       "      <th>Class_Eco Plus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>25</td>\n",
       "      <td>1880</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50001</th>\n",
       "      <td>25</td>\n",
       "      <td>2144</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50002</th>\n",
       "      <td>25</td>\n",
       "      <td>2068</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50003</th>\n",
       "      <td>25</td>\n",
       "      <td>1630</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50004</th>\n",
       "      <td>25</td>\n",
       "      <td>1358</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  Flight Distance  Seat comfort  Departure/Arrival time convenient  \\\n",
       "50000   25             1880             2                                  4   \n",
       "50001   25             2144             2                                  4   \n",
       "50002   25             2068             2                                  4   \n",
       "50003   25             1630             2                                  4   \n",
       "50004   25             1358             2                                  4   \n",
       "\n",
       "       Food and drink  Gate location  Inflight wifi service  \\\n",
       "50000               2              3                      4   \n",
       "50001               2              3                      3   \n",
       "50002               2              3                      3   \n",
       "50003               2              3                      1   \n",
       "50004               2              3                      2   \n",
       "\n",
       "       Inflight entertainment  Online support  Ease of Online booking  \\\n",
       "50000                       2               4                       4   \n",
       "50001                       2               1                       3   \n",
       "50002                       2               3                       3   \n",
       "50003                       2               1                       1   \n",
       "50004                       2               2                       2   \n",
       "\n",
       "       On-board service  Leg room service  Baggage handling  Checkin service  \\\n",
       "50000                 4                 2                 4                4   \n",
       "50001                 4                 5                 4                3   \n",
       "50002                 3                 4                 4                5   \n",
       "50003                 4                 5                 5                4   \n",
       "50004                 3                 4                 4                5   \n",
       "\n",
       "       Cleanliness  Online boarding  Departure Delay in Minutes  \\\n",
       "50000            5                4                           0   \n",
       "50001            5                3                           0   \n",
       "50002            5                3                           0   \n",
       "50003            4                1                           0   \n",
       "50004            4                2                          38   \n",
       "\n",
       "       Arrival Delay in Minutes  Gender_Female  Gender_Male  \\\n",
       "50000                       1.0              0            1   \n",
       "50001                       0.0              1            0   \n",
       "50002                       0.0              1            0   \n",
       "50003                       2.0              1            0   \n",
       "50004                      36.0              1            0   \n",
       "\n",
       "       Customer Type_Loyal Customer  Customer Type_disloyal Customer  \\\n",
       "50000                             0                                1   \n",
       "50001                             0                                1   \n",
       "50002                             0                                1   \n",
       "50003                             0                                1   \n",
       "50004                             0                                1   \n",
       "\n",
       "       Type of Travel_Business travel  Class_Business  Class_Eco  \\\n",
       "50000                               1               1          0   \n",
       "50001                               1               1          0   \n",
       "50002                               1               1          0   \n",
       "50003                               1               1          0   \n",
       "50004                               1               1          0   \n",
       "\n",
       "       Class_Eco Plus  \n",
       "50000               0  \n",
       "50001               0  \n",
       "50002               0  \n",
       "50003               0  \n",
       "50004               0  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_gd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5.label 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블 인코딩 후 데이터 : [0 1 0 ... 0 0 0]\n",
      "레이블 인코딩 클래스 확인 : ['dissatisfied' 'satisfied']\n",
      "레이블 인코딩을 디코딩 했을 때 : ['dissatisfied' 'satisfied' 'dissatisfied' ... 'dissatisfied'\n",
      " 'dissatisfied' 'dissatisfied']\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "# train_test_split 수행하기\n",
    "x_train, x_test, y_train ,y_test = train_test_split( x_gd, y, stratify=y, test_size=0.2, random_state=2023)\n",
    "\n",
    "# 레이블 인코더 생성하기\n",
    "le = LabelEncoder() \n",
    "\n",
    "# fit을 통해 y_train의 값마다 0과 1을 부여하는 규칙 생성하기\n",
    "le.fit(y_train) \n",
    "\n",
    "#y_train을 레이블 인코딩하기\n",
    "le_y_train = le.transform(y_train) \n",
    "\n",
    "#y_test를 레이블 인코딩하기\n",
    "le_y_test = le.transform(y_test) \n",
    "\n",
    "# 인코딩이 수행된 데이터 확인하기\n",
    "print('레이블 인코딩 후 데이터 :',le_y_train) \n",
    "\n",
    "# 라벨 별로 어떤 값이 부여되어 있는지 규칙 확인하기\n",
    "print('레이블 인코딩 클래스 확인 :',le.classes_) \n",
    "\n",
    "# 레이블 인코딩된 데이터를 디코딩했을 때 데이터 확인하기\n",
    "print('레이블 인코딩을 디코딩 했을 때 :',le.inverse_transform(le_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39887"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_y_train\n",
    "le_y_test\n",
    "sum(xgbc.predict(x_train) == le.inverse_transform(le_y_train))\n",
    "# len(xgbc.predict(x_train) == le.inverse_transform(le_y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-6.모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 18423, number of negative: 21464\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 939\n",
      "[LightGBM] [Info] Number of data points in the train set: 39887, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.461880 -> initscore=-0.152777\n",
      "[LightGBM] [Info] Start training from score -0.152777\n",
      "학습모델 :  LogisticRegression()\n",
      "train 정확도:  0.7619\n",
      "test 정확도:  0.76103\n",
      "----------------------------------\n",
      "학습모델 :  DecisionTreeClassifier(random_state=1)\n",
      "train 정확도:  1.0\n",
      "test 정확도:  0.92298\n",
      "----------------------------------\n",
      "학습모델 :  RandomForestClassifier(random_state=1)\n",
      "train 정확도:  1.0\n",
      "test 정확도:  0.95016\n",
      "----------------------------------\n",
      "학습모델 :  GradientBoostingClassifier(random_state=1)\n",
      "train 정확도:  0.92689\n",
      "test 정확도:  0.92529\n",
      "----------------------------------\n",
      "학습모델 :  XGBRFClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                colsample_bylevel=None, colsample_bytree=None,\n",
      "                early_stopping_rounds=None, enable_categorical=False,\n",
      "                eval_metric=None, feature_types=None, gamma=None, gpu_id=None,\n",
      "                grow_policy=None, importance_type=None,\n",
      "                interaction_constraints=None, max_bin=None,\n",
      "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "                max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "                min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "                n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "                objective='binary:logistic', predictor=None, random_state=1,\n",
      "                reg_alpha=None, ...)\n",
      "train 정확도:  0.90323\n",
      "test 정확도:  0.90403\n",
      "----------------------------------\n",
      "학습모델 :  ExtraTreesClassifier(random_state=1)\n",
      "train 정확도:  1.0\n",
      "test 정확도:  0.94595\n",
      "----------------------------------\n",
      "학습모델 :  LGBMClassifier(random_state=1)\n",
      "train 정확도:  0.95971\n",
      "test 정확도:  0.95207\n",
      "----------------------------------\n",
      "CPU times: total: 12.7 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 다양한 학습 모델 불러오기\n",
    "# scikit-learn 기반 모델 불러오기\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# scikit-learn 이외의 모델 불러오기\n",
    "from xgboost import XGBRFClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# 모델 생성하기\n",
    "lr = LogisticRegression()\n",
    "dtc = DecisionTreeClassifier(random_state=1)\n",
    "rfc = RandomForestClassifier(random_state=1)\n",
    "gbc = GradientBoostingClassifier(random_state=1)\n",
    "xgbc = XGBRFClassifier(random_state=1)\n",
    "etc = ExtraTreesClassifier(random_state=1)\n",
    "lgbmc = LGBMClassifier(random_state=1)\n",
    "\n",
    "# 모델 학습 수행하기\n",
    "lr.fit(x_train, le_y_train)\n",
    "dtc.fit(x_train, le_y_train)\n",
    "rfc.fit(x_train, le_y_train)\n",
    "gbc.fit(x_train, le_y_train)\n",
    "xgbc.fit(x_train, le_y_train)\n",
    "etc.fit(x_train, le_y_train)\n",
    "lgbmc.fit(x_train, le_y_train)\n",
    "\n",
    "# 순서대로 적용할 모델을 리스트에 저장하기\n",
    "models = [lr, dtc, rfc, gbc, xgbc, etc, lgbmc]\n",
    "\n",
    "# for문을 활용해 학습 모델 별 Score를 리스트에 저장하기\n",
    "acc_train_list = []\n",
    "acc_test_list = []\n",
    "for model in models:\n",
    "    acc_train_list.append(round(model.score(x_train, le_y_train),5))\n",
    "    acc_test_list.append(round(model.score(x_test, le_y_test),5))\n",
    "\n",
    "# 모델 별 정확도를 출력하기\n",
    "for i in range(len(models)):\n",
    "    print('학습모델 : ',models[i])\n",
    "    print('train 정확도: ',acc_train_list[i])\n",
    "    print('test 정확도: ',acc_test_list[i])\n",
    "    print('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-7. 최적의 하이퍼 파라미터 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-7-1. GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 파라미터 학습시 Score : 0.95046\n",
      "최적의 파라미터 : {'max_depth': 50, 'max_features': 'sqrt', 'n_estimators': 500}\n",
      "CPU times: total: 12 s\n",
      "Wall time: 8min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#GridSearchCV를 불러오기\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "# param_grid를 정의하여 각 파라미터 별로 교차해서 모든 학습을 수행하기\n",
    "param_grid = { \n",
    "    'n_estimators': [50 ,100, 200, 500],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'max_depth' : [10,20,30,40,50,None],\n",
    "}\n",
    "\n",
    "# estimator=rfc :가장 우수한 모델이었던 rfc 활용하기\n",
    "# param_grid=param_grid : 미리 정의한 파라미터들을 교차 적용하기\n",
    "# n_jobs=-1 : -1로 지정하면 모든 CPU 활용하기\n",
    "cv_rfc = GridSearchCV(\n",
    "    estimator=rfc, \n",
    "    param_grid=param_grid, \n",
    "    n_jobs=-1,\n",
    "    cv=5)\n",
    "\n",
    "# 학습 수행하기\n",
    "cv_rfc.fit(x_train, le_y_train) \n",
    "\n",
    "# best_score를 출력하기\n",
    "print('최적의 파라미터 학습시 Score :',round(cv_rfc.best_score_ , 5)) \n",
    "\n",
    "# best params을 출력하기\n",
    "print('최적의 파라미터 :',cv_rfc.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 모델의 Test 정확도: 1.0\n",
      "최적의 하이퍼 파라미터 반영한 모델의 Test 정확도:  0.93612\n",
      "정확도 향상:  -0.06388\n"
     ]
    }
   ],
   "source": [
    "# 최적의 하이퍼파라미터로 도출된 값을 모델에 입력하기\n",
    "rfc = RandomForestClassifier(\n",
    "    n_estimators = 50,\n",
    "    max_features = None,\n",
    "    n_jobs=-1,\n",
    "    max_depth = 10)\n",
    "\n",
    "# rfc 학습 수행하기\n",
    "rfc.fit(x_train, le_y_train) \n",
    "\n",
    "# 이전에 하이퍼파라미터 설정 없이 수행했을 때의 정확도 확인하기\n",
    "print('기존 모델의 Test 정확도: 1.0') \n",
    "\n",
    "# 최적의 하이퍼파라미터 입력 후 학습시킨 모델의 정확도 확인하기\n",
    "print('최적의 하이퍼 파라미터 반영한 모델의 Test 정확도: ',round(rfc.score(x_test, le_y_test),5)) \n",
    "\n",
    "# 향상된 정확도 gap 계산하기\n",
    "print('정확도 향상: ',round(rfc.score(x_test, le_y_test)-1.0 , 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-8. 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94      5366\n",
      "           1       0.92      0.94      0.93      4606\n",
      "\n",
      "    accuracy                           0.94      9972\n",
      "   macro avg       0.94      0.94      0.94      9972\n",
      "weighted avg       0.94      0.94      0.94      9972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# rfc 모델로 x_test를 예측한 값을 y_pred로 저장하기\n",
    "y_pred = rfc.predict(x_test) \n",
    "\n",
    "# 실제값 le_y_test(레이블 인코딩을 해야 0,1로 표현됨)와 예측값 y_pred 비교하기\n",
    "cm = confusion_matrix(le_y_test, y_pred, labels=[0,1])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "disp.plot()\n",
    "\n",
    "# 오차행렬(Confusion Matrix) 출력하기\n",
    "plt.show() \n",
    "\n",
    "# 오차행렬(Confusion Matrix)를 통한 각종 지표들을 리포트로 출력하기\n",
    "print(classification_report(le_y_test, y_pred)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
